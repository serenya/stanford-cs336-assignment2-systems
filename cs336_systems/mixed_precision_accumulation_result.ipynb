{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f2dfb8",
   "metadata": {},
   "source": [
    "Running code from mixed_precision_accumulation.py produces following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd03970",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tensor(10.0001)\n",
    "tensor(9.9531, dtype=torch.float16)\n",
    "tensor(10.0021)\n",
    "tensor(10.0021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22deabfc",
   "metadata": {},
   "source": [
    "According to source from the internet, above result is due to:\n",
    "\n",
    "With pure float32 accumulation, the result will be very close to exactly 10.0, since 0.01 is representable in fp32 with good precision.\n",
    "When accumulating float16 values, each 0.01 is already rounded (â‰ˆ0.0099945), so summing in fp16 or even summing fp16 values in fp32 produces a noticeable drift below 10.\n",
    "Casting the fp16 increment to fp32 before addition still preserves the initial rounding error, but avoids further accumulation error, giving the best result among the fp16-involved cases."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
